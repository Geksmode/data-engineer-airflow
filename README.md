# ðŸ› ï¸ Data Engineer Portfolio â€” Airflow ETL Pipeline

Bienvenue sur mon projet portfolio de Data Engineer. Ce projet dÃ©montre mes compÃ©tences en orchestration de donnÃ©es avec **Apache Airflow**, **Docker**, et une pipeline **CI/CD GitHub Actions**.

---

## ðŸš€ Objectif du projet

Ce pipeline simule une ingestion de donnÃ©es mÃ©tÃ©o, leur transformation, puis leur stockage local. Il illustre un cycle complet de type **ETL** :  
**Extract â†’ Transform â†’ Load** orchestrÃ© par Apache Airflow.

---

## ðŸ”§ Technologies utilisÃ©es

| Outil          | RÃ´le                                       |
|----------------|---------------------------------------------|
| ðŸ Python       | DÃ©veloppement des tÃ¢ches et transformation |
| ðŸ“¦ Docker       | Conteneurisation d'Airflow et PostgreSQL   |
| â° Apache Airflow | Orchestration des tÃ¢ches ETL               |
| ðŸ” GitHub Actions | DÃ©ploiement continu des DAGs               |
| ðŸ“„ Pandas       | Manipulation de donnÃ©es                     |

---

## ðŸ“‚ Structure du projet
data-engineer-airflow/
â”œâ”€â”€ dags/ # Fichiers DAGs Airflow
â”‚ â””â”€â”€ weather_etl.py # DAG principal (simulÃ©, sans API)
â”œâ”€â”€ output/ # CSVs gÃ©nÃ©rÃ©s par le DAG
â”œâ”€â”€ plugins/ # Plugins Airflow (vide ici)
â”œâ”€â”€ logs/ # Logs Airflow (non versionnÃ©s)
â”œâ”€â”€ .github/workflows/ # CI/CD GitHub Actions
â”‚ â””â”€â”€ deploy.yaml # Pipeline de dÃ©ploiement
â”œâ”€â”€ docker-compose.yaml # Stack Docker Airflow
â”œâ”€â”€ .env # Variables Docker (non versionnÃ©)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

## ðŸ§ª Exemple de pipeline ETL

Le DAG `weather_etl.py` contient trois tÃ¢ches :
1. **`extract_weather`** : gÃ©nÃ¨re alÃ©atoirement des donnÃ©es mÃ©tÃ©o (ville, tempÃ©rature, humiditÃ©)
2. **`transform_data`** : calcule la tempÃ©rature en Â°F
3. **`load_data`** : sauvegarde les rÃ©sultats dans un fichier CSV local

---

## âš™ï¸ Lancer le projet localement

```bash
# 1. Cloner le repo
git clone https://github.com/Geksmode/data-engineer-airflow.git
cd data-engineer-airflow

# 2. Initialiser Airflow et Docker
mkdir -p dags logs plugins output
echo "AIRFLOW_UID=$(id -u)" > .env
docker-compose up airflow-webserver  # puis Ctrl+C
docker-compose run airflow-webserver airflow db init
docker-compose up -d

# 3. AccÃ©der Ã  lâ€™interface Airflow
# http://localhost:8080 (admin / admin)

